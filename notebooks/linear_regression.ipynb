{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "#from sklearn.mixture import GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_boston\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "%matplotlib inline\n",
    "# plt.matplotlib.rcParams.update({'font.size': 50})\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'   \n",
    "plt.rcParams[\"patch.force_edgecolor\"] = False\n",
    "plt.rc('figure', titlesize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_PATH = '../augmented_datasets/pickles/hopkins_conf_gf0904_GDP_urban_weather.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopkins_confirmed = pd.read_pickle(PICKLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP</th>\n",
       "      <th>Urbanization</th>\n",
       "      <th>avg_m_tmp</th>\n",
       "      <th>avg_m_RH</th>\n",
       "      <th>avg_m_precip</th>\n",
       "      <th>avg_m_wind</th>\n",
       "      <th>Max_Cases</th>\n",
       "      <th>first_7</th>\n",
       "      <th>avg_interval_tmp</th>\n",
       "      <th>avg_interval_RH</th>\n",
       "      <th>...</th>\n",
       "      <th>3/29/2020</th>\n",
       "      <th>3/30/2020</th>\n",
       "      <th>3/31/2020</th>\n",
       "      <th>4/1/2020</th>\n",
       "      <th>4/2/2020</th>\n",
       "      <th>4/3/2020</th>\n",
       "      <th>4/4/2020</th>\n",
       "      <th>4/5/2020</th>\n",
       "      <th>4/6/2020</th>\n",
       "      <th>4/7/2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1070.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5295.000000</td>\n",
       "      <td>5308.000000</td>\n",
       "      <td>5322.000000</td>\n",
       "      <td>5336.000000</td>\n",
       "      <td>5336.000000</td>\n",
       "      <td>5328.000000</td>\n",
       "      <td>5320.000000</td>\n",
       "      <td>5331.000000</td>\n",
       "      <td>5327.000000</td>\n",
       "      <td>5331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52102.320042</td>\n",
       "      <td>74.298066</td>\n",
       "      <td>9.428750</td>\n",
       "      <td>70.922285</td>\n",
       "      <td>1.788514</td>\n",
       "      <td>11.821473</td>\n",
       "      <td>1686.908411</td>\n",
       "      <td>2.956743</td>\n",
       "      <td>12.753437</td>\n",
       "      <td>70.019905</td>\n",
       "      <td>...</td>\n",
       "      <td>180.928716</td>\n",
       "      <td>194.286890</td>\n",
       "      <td>214.074301</td>\n",
       "      <td>230.367822</td>\n",
       "      <td>250.538253</td>\n",
       "      <td>272.385893</td>\n",
       "      <td>298.439432</td>\n",
       "      <td>317.408923</td>\n",
       "      <td>336.277528</td>\n",
       "      <td>357.665864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21063.174970</td>\n",
       "      <td>14.606890</td>\n",
       "      <td>7.587477</td>\n",
       "      <td>9.037325</td>\n",
       "      <td>1.682517</td>\n",
       "      <td>3.680288</td>\n",
       "      <td>14944.506938</td>\n",
       "      <td>1.775053</td>\n",
       "      <td>7.341610</td>\n",
       "      <td>10.104754</td>\n",
       "      <td>...</td>\n",
       "      <td>3067.974341</td>\n",
       "      <td>3365.232865</td>\n",
       "      <td>3733.095866</td>\n",
       "      <td>4097.744440</td>\n",
       "      <td>4518.005728</td>\n",
       "      <td>4970.220197</td>\n",
       "      <td>5494.354993</td>\n",
       "      <td>5883.691669</td>\n",
       "      <td>6295.815448</td>\n",
       "      <td>6725.900356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>396.000000</td>\n",
       "      <td>14.338000</td>\n",
       "      <td>-16.824675</td>\n",
       "      <td>11.831169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.007246</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.926087</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.200000</td>\n",
       "      <td>-12.600000</td>\n",
       "      <td>-12.900000</td>\n",
       "      <td>-14.700000</td>\n",
       "      <td>-18.700000</td>\n",
       "      <td>-18.100000</td>\n",
       "      <td>-13.800000</td>\n",
       "      <td>-10.200000</td>\n",
       "      <td>-8.700000</td>\n",
       "      <td>-10.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46609.000000</td>\n",
       "      <td>66.300000</td>\n",
       "      <td>3.945685</td>\n",
       "      <td>66.922078</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>9.324675</td>\n",
       "      <td>36.250000</td>\n",
       "      <td>1.707143</td>\n",
       "      <td>6.865476</td>\n",
       "      <td>66.542857</td>\n",
       "      <td>...</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.475000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55172.000000</td>\n",
       "      <td>75.100000</td>\n",
       "      <td>8.482468</td>\n",
       "      <td>72.571429</td>\n",
       "      <td>1.601016</td>\n",
       "      <td>11.419895</td>\n",
       "      <td>86.500000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>12.008036</td>\n",
       "      <td>71.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.995000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>18.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61594.000000</td>\n",
       "      <td>86.200000</td>\n",
       "      <td>13.866234</td>\n",
       "      <td>76.493506</td>\n",
       "      <td>2.681818</td>\n",
       "      <td>14.035146</td>\n",
       "      <td>306.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.847756</td>\n",
       "      <td>76.250940</td>\n",
       "      <td>...</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200277.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>32.323377</td>\n",
       "      <td>88.608696</td>\n",
       "      <td>7.818052</td>\n",
       "      <td>29.774026</td>\n",
       "      <td>396223.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>35.100000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>140909.000000</td>\n",
       "      <td>161837.000000</td>\n",
       "      <td>188172.000000</td>\n",
       "      <td>213372.000000</td>\n",
       "      <td>243616.000000</td>\n",
       "      <td>275586.000000</td>\n",
       "      <td>308850.000000</td>\n",
       "      <td>337072.000000</td>\n",
       "      <td>366667.000000</td>\n",
       "      <td>396223.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 GDP  Urbanization    avg_m_tmp     avg_m_RH  avg_m_precip  \\\n",
       "count    1070.000000   1070.000000  1070.000000  1070.000000   1070.000000   \n",
       "mean    52102.320042     74.298066     9.428750    70.922285      1.788514   \n",
       "std     21063.174970     14.606890     7.587477     9.037325      1.682517   \n",
       "min       396.000000     14.338000   -16.824675    11.831169      0.000000   \n",
       "25%     46609.000000     66.300000     3.945685    66.922078      0.032922   \n",
       "50%     55172.000000     75.100000     8.482468    72.571429      1.601016   \n",
       "75%     61594.000000     86.200000    13.866234    76.493506      2.681818   \n",
       "max    200277.000000    100.000000    32.323377    88.608696      7.818052   \n",
       "\n",
       "        avg_m_wind      Max_Cases      first_7  avg_interval_tmp  \\\n",
       "count  1070.000000    1070.000000  1063.000000       1070.000000   \n",
       "mean     11.821473    1686.908411     2.956743         12.753437   \n",
       "std       3.680288   14944.506938     1.775053          7.341610   \n",
       "min       3.007246      20.000000     0.000000         -9.926087   \n",
       "25%       9.324675      36.250000     1.707143          6.865476   \n",
       "50%      11.419895      86.500000     2.666667         12.008036   \n",
       "75%      14.035146     306.750000     4.000000         17.847756   \n",
       "max      29.774026  396223.000000    11.000000         35.100000   \n",
       "\n",
       "       avg_interval_RH  ...      3/29/2020      3/30/2020      3/31/2020  \\\n",
       "count      1070.000000  ...    5295.000000    5308.000000    5322.000000   \n",
       "mean         70.019905  ...     180.928716     194.286890     214.074301   \n",
       "std          10.104754  ...    3067.974341    3365.232865    3733.095866   \n",
       "min           8.444444  ...     -13.200000     -12.600000     -12.900000   \n",
       "25%          66.542857  ...       6.350000       5.500000       6.100000   \n",
       "50%          71.666667  ...      16.000000      14.100000      14.000000   \n",
       "75%          76.250940  ...      51.000000      47.000000      59.000000   \n",
       "max          94.000000  ...  140909.000000  161837.000000  188172.000000   \n",
       "\n",
       "            4/1/2020       4/2/2020       4/3/2020       4/4/2020  \\\n",
       "count    5336.000000    5336.000000    5328.000000    5320.000000   \n",
       "mean      230.367822     250.538253     272.385893     298.439432   \n",
       "std      4097.744440    4518.005728    4970.220197    5494.354993   \n",
       "min       -14.700000     -18.700000     -18.100000     -13.800000   \n",
       "25%         5.300000       4.300000       4.475000       4.500000   \n",
       "50%        13.900000      15.000000      15.800000      14.995000   \n",
       "75%        58.000000      54.000000      54.000000      60.000000   \n",
       "max    213372.000000  243616.000000  275586.000000  308850.000000   \n",
       "\n",
       "            4/5/2020       4/6/2020       4/7/2020  \n",
       "count    5331.000000    5327.000000    5331.000000  \n",
       "mean      317.408923     336.277528     357.665864  \n",
       "std      5883.691669    6295.815448    6725.900356  \n",
       "min       -10.200000      -8.700000     -10.300000  \n",
       "25%         5.000000       5.200000       7.000000  \n",
       "50%        15.100000      16.800000      18.800000  \n",
       "75%        64.000000      60.000000      64.000000  \n",
       "max    337072.000000  366667.000000  396223.000000  \n",
       "\n",
       "[8 rows x 90 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopkins_confirmed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    \"\"\"\n",
    "    Perform mean normalization on the features and true labels.\n",
    "    \"\"\"\n",
    "    X = (X - X.mean()) / (X.max() - X.min())\n",
    "    y = (y - y.mean()) / (y.max() - y.min())\n",
    "    return X, y\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the average squared difference between an obserbation's actual and\n",
    "    predicted values for linear regression.  \n",
    "    \"\"\"   \n",
    "    J = 0\n",
    "    m = X.shape[0]\n",
    "    J = (1 / ( 2 * m )) * ((( (theta * X).sum(axis=1) - y) ** 2 ).sum() )\n",
    "    return J\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of the model using gradient descent using \n",
    "    the *training set*. Gradient descent is an optimization algorithm \n",
    "    used to minimize some (loss) function by iteratively moving in \n",
    "    the direction of steepest descent as defined by the negative of \n",
    "    the gradient. We use gradient descent to update the parameters\n",
    "    (weights) of our model.\n",
    "    \"\"\"\n",
    "    J_history = []\n",
    "    theta = theta.copy()\n",
    "    m = X.shape[0]\n",
    "    for iter_ in range(num_iters):\n",
    "        tmp_theta = theta.copy()\n",
    "        for j in range(theta.shape[0]):\n",
    "            tmp_theta[j] = theta[j] - (alpha /  m ) * ( ((theta * X).sum(axis=1) - y) * X.T[j]).sum() \n",
    "        theta = tmp_theta.copy()\n",
    "        J_history.append(compute_cost(X, y, theta))\n",
    "    return tmp_theta, J_history\n",
    "\n",
    "def pinv(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the optimal values of the parameters using the pseudoinverse\n",
    "    approach as you saw in class using the *training set*.\n",
    "    \"\"\"\n",
    "    pinv_theta = []\n",
    "    pinv_theta = (np.linalg.inv(X_train.T @ X_train) @ X_train.T) @ y_train\n",
    "    return pinv_theta\n",
    "\n",
    "def efficient_gradient_descent(X, y, theta, alpha, num_iters):   \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "    THRESHOLD = 10 ** -8\n",
    "    m = X.shape[0]\n",
    "    i = 0\n",
    "    while i <  num_iters:\n",
    "        tmp_theta = theta.copy()\n",
    "        for j in range(theta.shape[0]):\n",
    "            tmp_theta[j] = theta[j] - (alpha /  m ) * ( ((theta * X).sum(axis=1) - y) * X.T[j]).sum() \n",
    "        theta = tmp_theta.copy()\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        if i > 0:\n",
    "            if (J_history[-1] - cost) < THRESHOLD:\n",
    "                i = num_iters + 1\n",
    "        J_history.append(cost)\n",
    "        i += 1\n",
    "    return theta, J_history\n",
    "\n",
    "def find_best_alpha(X_train, y_train, X_val, y_val, iterations):\n",
    "    \"\"\"\n",
    "    Iterate over provided values of alpha and train a model using the \n",
    "    *training* dataset. maintain a python dictionary with alpha as the \n",
    "    key and the loss on the *validation* set as the value.\n",
    "    \"\"\"\n",
    "    alphas = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 2, 3]\n",
    "    alpha_dict = {}\n",
    "    np.random.seed(42) \n",
    "    theta = np.random.random(size=2)\n",
    "    for alpha in alphas:\n",
    "        computed_theta, _ = efficient_gradient_descent(X_train, y_train, theta, alpha, iterations)\n",
    "        alpha_dict[alpha] = compute_cost(X_val, y_val, computed_theta)\n",
    "    return alpha_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut off of outlier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['GDP', 'Urbanization', 'avg_interval_tmp', 'avg_interval_RH']\n",
    "target_columns = ['first_7', 'GF_Q1', 'GF_Q2', 'GF_Q3']\n",
    "X = hopkins_confirmed[feature_columns].copy().dropna().values\n",
    "y = hopkins_confirmed[target_columns].copy().dropna().values\n",
    "len(hopkins_confirmed[feature_columns])\n",
    "len(hopkins_confirmed[target_columns])\n",
    "\n",
    "# Now we see that X != y\n",
    "len(X)\n",
    "len(y)\n",
    "\n",
    "\n",
    "temp_hopkins = hopkins_confirmed.dropna().copy()\n",
    "X = temp_hopkins[feature_columns].copy().dropna().values\n",
    "y = temp_hopkins[target_columns].copy().dropna().values\n",
    "len(X)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['GDP', 'Urbanization', 'avg_interval_tmp', 'avg_interval_RH']\n",
    "target_columns = ['first_7', 'GF_Q1', 'GF_Q2', 'GF_Q3']\n",
    "X = hopkins_confirmed[feature_columns].copy().dropna().values\n",
    "y = hopkins_confirmed[target_columns].copy().dropna().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we have some nan values in some of the target columns, so we provide a temporary walkaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5350"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5350"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hopkins_confirmed[feature_columns])\n",
    "len(hopkins_confirmed[target_columns])\n",
    "len(X)\n",
    "len(y)\n",
    "\n",
    "temp_hopkins = hopkins_confirmed.dropna().copy()\n",
    "X = temp_hopkins[feature_columns].copy().dropna().values\n",
    "y = temp_hopkins[target_columns].copy().dropna().values\n",
    "len(X)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Regression - Interval temp vs first_7 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Regression - All params vs first_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import tree, metrics\n",
    "\n",
    "def train_test_split(X, n_splits=5):\n",
    "    '''\n",
    "    Splits rows into training indices and test indices.\n",
    "    :param X: numpy array of training data, e.g.  np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) - each sample has two features\n",
    "    :return: Returns indices of rows for train and test for n_splits. e.g. n_splits=2: \n",
    "    train_folds = [[0,2,3], [1,2,3]] test_folds = [[1], [0]] \n",
    "    '''\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, random_state=2346, shuffle=True)\n",
    "    kf.get_n_splits(X)\n",
    "    \n",
    "    train_folds, test_folds = [], []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        train_folds.append(train_index)\n",
    "        test_folds.append(test_index)\n",
    "    \n",
    "    return train_folds, test_folds\n",
    "\n",
    "\n",
    "def decision_tree_train(X_train, y_train):\n",
    "    dt = tree.DecisionTreeRegressor()\n",
    "    trained_model = dt.fit(X_train, y_train)\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def linear_regression_train(X_train, y_train):\n",
    "    # TODO - can use sklearn linear regression package. Should also have fit (like in decision_tree_train)\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "## Example: ##\n",
    "\n",
    "# Settings:\n",
    "\n",
    "# Desired data we wish to train-test on - needs to be a pandas data frame formatted like 'colds' or 'hots':\n",
    "data = colds\n",
    "feature_cols = ['avg_interval_tmp']  # TODO - try adding more features (GDP, urban data...)\n",
    "label_col = ['GF_Q3']\n",
    "\n",
    "# Model function\n",
    "model_fn = decision_tree_train  # can be replaced by linear regression\n",
    "\n",
    "# Metric used to assess the model:\n",
    "metric_fn = metrics.mean_squared_error\n",
    "\n",
    "# Training-testing (nothing to change beyond this point):\n",
    "\n",
    "\n",
    "X, y = np.array(data[feature_cols]), np.array(data[label_col])\n",
    "\n",
    "# Split the data into train, test for n_splits train-test rounds\n",
    "train_folds, test_folds = train_test_split(X, n_splits=5)\n",
    "\n",
    "# Train-test the model for each of the n_splits:\n",
    "for train_test_round in range(len(train_folds)):\n",
    "    train_index = train_folds[train_test_round]\n",
    "    test_index = test_folds[train_test_round]\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt_trained = decision_tree_train(X_train, y_train)\n",
    "    if model_fn == decision_tree_train: \n",
    "        tree.plot_tree(dt_trained, feature_names=feature_cols)\n",
    "    # evaluate on test\n",
    "    y_pred = dt_trained.predict(X_test)\n",
    "    print(metric_fn(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
